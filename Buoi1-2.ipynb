{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataFrame-Ex\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Avg: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/SampleData/TCB_2018_2020.csv\"\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+-----+------+---------+\n",
      "|      Date| High|  Low| Open|Close|   Avg|   Volume|\n",
      "+----------+-----+-----+-----+-----+------+---------+\n",
      "|2018-06-04|105.0|102.4|102.4|102.4|102.49|2811840.0|\n",
      "|2018-06-05|106.0| 96.0| 99.1| 96.0|100.19|1689500.0|\n",
      "|2018-06-06| 96.0| 91.0| 95.0| 92.0| 92.98|1901680.0|\n",
      "|2018-06-07| 98.4| 93.1| 94.5| 98.4|  97.0|1476540.0|\n",
      "|2018-06-08|105.2| 99.5|101.0|105.2|103.83|2008500.0|\n",
      "+----------+-----+-----+-----+-----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType, DateType\n",
    "schema = StructType([\n",
    "    StructField(name=\"Date\", dataType=DateType(), nullable=True),\n",
    "    StructField(name=\"High\", dataType=DoubleType(), nullable=True),\n",
    "    StructField(name=\"Low\", dataType=DoubleType(), nullable=True),\n",
    "    StructField(name=\"Open\", dataType=DoubleType(), nullable=True),\n",
    "    StructField(name=\"Close\", dataType=DoubleType(), nullable=True),\n",
    "    StructField(name=\"Avg\", dataType=DoubleType(), nullable=True),\n",
    "    StructField(name=\"Volume\", dataType=DoubleType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(file_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Avg: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"data/SampleData/iris.json\"\n",
    "df = spark.read.json(json_file_path, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- petalLength: double (nullable = true)\n",
      " |-- petalWidth: double (nullable = true)\n",
      " |-- sepalLength: double (nullable = true)\n",
      " |-- sepalWidth: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-------+\n",
      "|petalLength|petalWidth|sepalLength|sepalWidth|species|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "|        1.4|       0.2|        5.1|       3.5| setosa|\n",
      "|        1.4|       0.2|        4.9|       3.0| setosa|\n",
      "|        1.3|       0.2|        4.7|       3.2| setosa|\n",
      "|        1.5|       0.2|        4.6|       3.1| setosa|\n",
      "|        1.4|       0.2|        5.0|       3.6| setosa|\n",
      "+-----------+----------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
